{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Wordrank and Word2Vec\n",
    "\n",
    "Wordrank is a fresh new approach to the word embeddings, which formulates it as a ranking problem. That is, given a word w, it aims to output an ordered list (c1, c2, · · ·) of context words such that words that co-occur with w appear at the top of the list. This formulation fits naturally to popular word embedding tasks such as word similarity/analogy since instead of the likelihood of each word, we are interested in finding the most relevant words\n",
    "in a given context.\n",
    "\n",
    "Gensim is used to train the word2vec models, and analogical reasoning task is used for comparing the models. Word2vec and FastText embeddings are trained using the skipgram architecture here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/parul/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_multiple_whitespaces\n",
    "\n",
    "# Only the brown corpus is needed in case you don't have it.\n",
    "nltk.download('brown') \n",
    "\n",
    "# Generate brown corpus text file\n",
    "with open('brown_corp.txt', 'w+') as f:\n",
    "    for word in nltk.corpus.brown.words():\n",
    "        f.write('{word} '.format(word=word))\n",
    "    f.seek(0)\n",
    "    brown = f.read()\n",
    "\n",
    "# Preprocess brown corpus\n",
    "with open('proc_brown_corp.txt', 'w') as f:\n",
    "    proc_brown = strip_punctuation(brown)\n",
    "    proc_brown = strip_multiple_whitespaces(proc_brown).lower()\n",
    "    f.write(proc_brown)\n",
    "\n",
    "# Set WR_HOME and FT_HOME to respective directory root\n",
    "WR_HOME = 'wordrank/'\n",
    "FT_HOME = 'fastText/'\n",
    "\n",
    "# download the text8 corpus (a 100 MB sample of preprocessed wikipedia text)\n",
    "import os.path\n",
    "if not os.path.isfile('text8'):\n",
    "    !wget -c http://mattmahoney.net/dc/text8.zip\n",
    "    !unzip text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train Models\n",
    "For training the models yourself, you'll need to have Gensim, FastText and Wordrank set up on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec on proc_brown_corp.txt corpus..\n",
      "CPU times: user 1min 7s, sys: 527 ms, total: 1min 8s\n",
      "Wall time: 46.8 s\n",
      "\n",
      "Saved gensim model as brown_gs.vec\n",
      "Training fasttext on proc_brown_corp.txt corpus..\n",
      "Read 1M words\n",
      "Number of words:  14042\n",
      "Number of labels: 0\n",
      "Progress: 99.6%  words/sec/thread: 58810  lr: 0.000179  loss: 2.348125  eta: 0h0m Progress: 20.1%  words/sec/thread: 30702  lr: 0.039934  loss: 2.296231  eta: 0h0m Progress: 100.0%  words/sec/thread: 58810  lr: 0.000000  loss: 2.348125  eta: 0h0m \n",
      "CPU times: user 842 ms, sys: 284 ms, total: 1.13 s\n",
      "Wall time: 41.3 s\n",
      "\n",
      "Training wordrank on proc_brown_corp.txt corpus..\n",
      "CPU times: user 10.8 s, sys: 1.02 s, total: 11.8 s\n",
      "Wall time: 8h 24min 25s\n",
      "\n",
      "Saved wordrank model as brown_wr.vec\n",
      "\n",
      "Loading ensemble embeddings (vector combination of word and context embeddings)..\n",
      "CPU times: user 8.97 s, sys: 279 ms, total: 9.25 s\n",
      "Wall time: 13.8 s\n",
      "\n",
      "Saved wordrank (ensemble) model as brown_wr_ensemble.vec\n"
     ]
    }
   ],
   "source": [
    "MODELS_DIR = 'models/'\n",
    "!mkdir -p {MODELS_DIR}\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.wrappers import Wordrank\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# fasttext params\n",
    "lr = 0.05\n",
    "dim = 100\n",
    "ws = 5\n",
    "epoch = 5\n",
    "minCount = 5\n",
    "neg = 5\n",
    "loss = 'ns'\n",
    "t = 1e-4\n",
    "\n",
    "w2v_params = {\n",
    "    'alpha': 0.025,\n",
    "    'size': 100,\n",
    "    'window': 15,\n",
    "    'iter': 5,\n",
    "    'min_count': 5,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': 5\n",
    "}\n",
    "\n",
    "wr_params = {\n",
    "    'size': 100,\n",
    "    'window': 15,\n",
    "    'iter': 91,\n",
    "    'min_count': 5\n",
    "}\n",
    "\n",
    "def train_models(corpus_file, output_name):\n",
    "    # Train using word2vec\n",
    "    output_file = '{:s}_gs'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n",
    "        print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n",
    "        # Text8Corpus class for reading space-separated words file\n",
    "        %time gs_model = Word2Vec(Text8Corpus(corpus_file), **w2v_params); gs_model\n",
    "        locals()['gs_model'].save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n",
    "        print('\\nSaved gensim model as {:s}.vec'.format(output_file))\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n",
    "\n",
    "    # Train using fasttext\n",
    "    output_file = '{:s}_ft'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n",
    "        print('Training fasttext on {:s} corpus..'.format(corpus_file))\n",
    "        %time !{FT_HOME}fasttext skipgram -input {corpus_file} -output {MODELS_DIR+output_file}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n",
    "        \n",
    "    # Train using wordrank\n",
    "    output_file = '{:s}_wr'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n",
    "        print('\\nTraining wordrank on {:s} corpus..'.format(corpus_file))\n",
    "        %time wr_model = Wordrank.train(WR_HOME, corpus_file, **wr_params); wr_model\n",
    "        locals()['wr_model'].save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n",
    "        print('\\nSaved wordrank model as {:s}.vec'.format(output_file))\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n",
    "     \n",
    "    # Loading ensemble embeddings\n",
    "    output_file = '{:s}_wr_ensemble'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n",
    "        print('\\nLoading ensemble embeddings (vector combination of word and context embeddings)..')\n",
    "        %time wr_model = Wordrank.load_wordrank_model(os.path.join(WR_HOME, 'model/wordrank.words'), os.path.join(WR_HOME, 'model/meta/vocab.txt'), os.path.join(WR_HOME, 'model/wordrank.contexts'), sorted_vocab=1, ensemble=1); wr_model\n",
    "        locals()['wr_model'].save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n",
    "        print('\\nSaved wordrank (ensemble) model as {:s}.vec'.format(output_file))\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n",
    "                \n",
    "train_models(corpus_file='proc_brown_corp.txt', output_name='brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec on text8 corpus..\n",
      "CPU times: user 24min 21s, sys: 8.64 s, total: 24min 29s\n",
      "Wall time: 18min 33s\n",
      "\n",
      "Saved gensim model as text8_gs.vec\n",
      "\n",
      "Using existing model file text8_ft.vec\n",
      "\n",
      "Using existing model file text8_wr.vec\n",
      "\n",
      "Using existing model file text8_wr_ensemble.vec\n"
     ]
    }
   ],
   "source": [
    "train_models(corpus_file='text8', output_name='text8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train wordrank model using ensemble in second case as it is known to give a small performance boost in some cases. So we'll test accuracy for both the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def print_accuracy(model, questions_file):\n",
    "    print('Evaluating...\\n')\n",
    "    acc = model.accuracy(questions_file)\n",
    "\n",
    "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
    "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
    "    sem_acc = 100*float(sem_correct)/sem_total\n",
    "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
    "    \n",
    "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
    "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
    "    syn_acc = 100*float(syn_correct)/syn_total\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 13:58:37,691 : INFO : loading projection weights from models/brown_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Gensim embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 13:58:45,351 : INFO : loaded (14042, 100) matrix from models/brown_gs.vec\n",
      "2016-12-27 13:58:45,418 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Word2Vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 13:58:45,823 : INFO : capital-common-countries: 0.0% (0/90)\n",
      "2016-12-27 13:58:46,182 : INFO : capital-world: 0.0% (0/53)\n",
      "2016-12-27 13:58:46,300 : INFO : currency: 0.0% (0/12)\n",
      "2016-12-27 13:58:48,230 : INFO : city-in-state: 0.9% (4/457)\n",
      "2016-12-27 13:58:49,250 : INFO : family: 20.0% (48/240)\n",
      "2016-12-27 13:58:53,225 : INFO : gram1-adjective-to-adverb: 0.1% (1/812)\n",
      "2016-12-27 13:58:54,105 : INFO : gram2-opposite: 0.0% (0/132)\n",
      "2016-12-27 13:58:59,197 : INFO : gram3-comparative: 1.8% (19/1056)\n",
      "2016-12-27 13:59:00,302 : INFO : gram4-superlative: 0.5% (1/210)\n",
      "2016-12-27 13:59:03,744 : INFO : gram5-present-participle: 2.6% (17/650)\n",
      "2016-12-27 13:59:05,319 : INFO : gram6-nationality-adjective: 11.4% (34/297)\n",
      "2016-12-27 13:59:11,041 : INFO : gram7-past-tense: 3.3% (42/1260)\n",
      "2016-12-27 13:59:14,074 : INFO : gram8-plural: 6.6% (46/702)\n",
      "2016-12-27 13:59:15,578 : INFO : gram9-plural-verbs: 2.0% (7/342)\n",
      "2016-12-27 13:59:15,579 : INFO : total: 3.5% (219/6313)\n",
      "2016-12-27 13:59:15,586 : INFO : loading projection weights from models/brown_ft.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 52/852, Accuracy: 6.10%\n",
      "Syntactic: 167/5461, Accuracy: 3.06%\n",
      "\n",
      "\n",
      "Loading FastText embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 13:59:21,713 : INFO : loaded (14042, 100) matrix from models/brown_ft.vec\n",
      "2016-12-27 13:59:21,775 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FastText:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 13:59:22,205 : INFO : capital-common-countries: 1.1% (1/90)\n",
      "2016-12-27 13:59:22,546 : INFO : capital-world: 0.0% (0/53)\n",
      "2016-12-27 13:59:22,651 : INFO : currency: 0.0% (0/12)\n",
      "2016-12-27 13:59:24,741 : INFO : city-in-state: 2.4% (11/457)\n",
      "2016-12-27 13:59:25,821 : INFO : family: 11.7% (28/240)\n",
      "2016-12-27 13:59:29,840 : INFO : gram1-adjective-to-adverb: 79.9% (649/812)\n",
      "2016-12-27 13:59:30,628 : INFO : gram2-opposite: 79.5% (105/132)\n",
      "2016-12-27 13:59:35,972 : INFO : gram3-comparative: 56.3% (595/1056)\n",
      "2016-12-27 13:59:37,289 : INFO : gram4-superlative: 71.4% (150/210)\n",
      "2016-12-27 13:59:40,495 : INFO : gram5-present-participle: 65.7% (427/650)\n",
      "2016-12-27 13:59:41,863 : INFO : gram6-nationality-adjective: 35.0% (104/297)\n",
      "2016-12-27 13:59:47,878 : INFO : gram7-past-tense: 12.1% (153/1260)\n",
      "2016-12-27 13:59:51,524 : INFO : gram8-plural: 53.1% (373/702)\n",
      "2016-12-27 13:59:53,380 : INFO : gram9-plural-verbs: 69.0% (236/342)\n",
      "2016-12-27 13:59:53,382 : INFO : total: 44.9% (2832/6313)\n",
      "2016-12-27 13:59:53,389 : INFO : loading projection weights from models/brown_wr.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 40/852, Accuracy: 4.69%\n",
      "Syntactic: 2792/5461, Accuracy: 51.13%\n",
      "\n",
      "\n",
      "Loading Wordrank embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 14:00:00,114 : INFO : loaded (14042, 100) matrix from models/brown_wr.vec\n",
      "2016-12-27 14:00:00,173 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Wordrank:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 14:00:00,438 : INFO : capital-common-countries: 10.0% (9/90)\n",
      "2016-12-27 14:00:00,694 : INFO : capital-world: 15.1% (8/53)\n",
      "2016-12-27 14:00:00,762 : INFO : currency: 0.0% (0/12)\n",
      "2016-12-27 14:00:02,165 : INFO : city-in-state: 8.1% (37/457)\n",
      "2016-12-27 14:00:02,909 : INFO : family: 23.8% (57/240)\n",
      "2016-12-27 14:00:05,119 : INFO : gram1-adjective-to-adverb: 0.6% (5/812)\n",
      "2016-12-27 14:00:05,616 : INFO : gram2-opposite: 0.0% (0/132)\n",
      "2016-12-27 14:00:09,606 : INFO : gram3-comparative: 2.0% (21/1056)\n",
      "2016-12-27 14:00:10,392 : INFO : gram4-superlative: 1.0% (2/210)\n",
      "2016-12-27 14:00:12,894 : INFO : gram5-present-participle: 0.5% (3/650)\n",
      "2016-12-27 14:00:14,405 : INFO : gram6-nationality-adjective: 10.8% (32/297)\n",
      "2016-12-27 14:00:18,084 : INFO : gram7-past-tense: 1.6% (20/1260)\n",
      "2016-12-27 14:00:20,194 : INFO : gram8-plural: 8.3% (58/702)\n",
      "2016-12-27 14:00:21,221 : INFO : gram9-plural-verbs: 0.3% (1/342)\n",
      "2016-12-27 14:00:21,222 : INFO : total: 4.0% (253/6313)\n",
      "2016-12-27 14:00:21,229 : INFO : loading projection weights from models/brown_wr_ensemble.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 111/852, Accuracy: 13.03%\n",
      "Syntactic: 142/5461, Accuracy: 2.60%\n",
      "\n",
      "\n",
      "Loading Wordrank ensemble embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 14:00:25,328 : INFO : loaded (14042, 100) matrix from models/brown_wr_ensemble.vec\n",
      "2016-12-27 14:00:25,413 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Wordrank:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 14:00:25,898 : INFO : capital-common-countries: 14.4% (13/90)\n",
      "2016-12-27 14:00:26,340 : INFO : capital-world: 18.9% (10/53)\n",
      "2016-12-27 14:00:26,469 : INFO : currency: 0.0% (0/12)\n",
      "2016-12-27 14:00:28,738 : INFO : city-in-state: 8.3% (38/457)\n",
      "2016-12-27 14:00:29,890 : INFO : family: 28.8% (69/240)\n",
      "2016-12-27 14:00:33,588 : INFO : gram1-adjective-to-adverb: 0.6% (5/812)\n",
      "2016-12-27 14:00:34,111 : INFO : gram2-opposite: 0.0% (0/132)\n",
      "2016-12-27 14:00:38,860 : INFO : gram3-comparative: 3.4% (36/1056)\n",
      "2016-12-27 14:00:40,451 : INFO : gram4-superlative: 0.0% (0/210)\n",
      "2016-12-27 14:00:44,070 : INFO : gram5-present-participle: 1.7% (11/650)\n",
      "2016-12-27 14:00:45,958 : INFO : gram6-nationality-adjective: 16.8% (50/297)\n",
      "2016-12-27 14:00:52,447 : INFO : gram7-past-tense: 3.8% (48/1260)\n",
      "2016-12-27 14:00:56,598 : INFO : gram8-plural: 11.1% (78/702)\n",
      "2016-12-27 14:00:58,343 : INFO : gram9-plural-verbs: 0.9% (3/342)\n",
      "2016-12-27 14:00:58,344 : INFO : total: 5.7% (361/6313)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 130/852, Accuracy: 15.26%\n",
      "Syntactic: 231/5461, Accuracy: 4.23%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_analogies_file = './datasets/questions-words.txt'\n",
    "\n",
    "print('\\nLoading Gensim embeddings')\n",
    "brown_gs = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\n",
    "print('Accuracy for Word2Vec:')\n",
    "print_accuracy(brown_gs, word_analogies_file)\n",
    "\n",
    "print('\\nLoading FastText embeddings')\n",
    "brown_ft = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_ft.vec')\n",
    "print('Accuracy for FastText:')\n",
    "print_accuracy(brown_ft, word_analogies_file)\n",
    "\n",
    "print('\\nLoading Wordrank embeddings')\n",
    "brown_wr = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_wr.vec')\n",
    "print('Accuracy for Wordrank:')\n",
    "print_accuracy(brown_wr, word_analogies_file)\n",
    "\n",
    "print('\\nLoading Wordrank ensemble embeddings')\n",
    "brown_wr_ensemble = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_wr_ensemble.vec')\n",
    "print('Accuracy for Wordrank:')\n",
    "print_accuracy(brown_wr_ensemble, word_analogies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the above outputs, wordrank performs better in semantic analogies, whereas, fasttext performs significantly better in syntactic analogies. Also ensemble embeddings gives a small performance boost in wordrank's case.\n",
    "\n",
    "Wordrank's effectiveness in Semantic analogies is possibly due to it's focused attention on getting most relevant words right at the top using the ranking approach.\n",
    "And as fasttext is designed to incorporate morphological information about words, it results in it's performance boost in syntactic analogies, as most of the syntactic analogies are morphology based.\n",
    "\n",
    "Now lets evaluate on a larger corpus, text8, and see how it effects the performance of different embedding models.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:10:28,984 : INFO : loading projection weights from models/text8_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:10:36,842 : INFO : loaded (71290, 100) matrix from models/text8_gs.vec\n",
      "2016-12-27 18:10:36,908 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for word2vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:10:38,821 : INFO : capital-common-countries: 68.6% (347/506)\n",
      "2016-12-27 18:10:44,002 : INFO : capital-world: 52.3% (760/1452)\n",
      "2016-12-27 18:10:44,976 : INFO : currency: 19.8% (53/268)\n",
      "2016-12-27 18:10:50,552 : INFO : city-in-state: 24.8% (389/1571)\n",
      "2016-12-27 18:10:51,692 : INFO : family: 47.7% (146/306)\n",
      "2016-12-27 18:10:55,175 : INFO : gram1-adjective-to-adverb: 18.0% (136/756)\n",
      "2016-12-27 18:10:56,535 : INFO : gram2-opposite: 13.4% (41/306)\n",
      "2016-12-27 18:11:01,901 : INFO : gram3-comparative: 37.8% (476/1260)\n",
      "2016-12-27 18:11:03,798 : INFO : gram4-superlative: 22.3% (113/506)\n",
      "2016-12-27 18:11:07,566 : INFO : gram5-present-participle: 22.9% (227/992)\n",
      "2016-12-27 18:11:12,830 : INFO : gram6-nationality-adjective: 86.7% (1188/1371)\n",
      "2016-12-27 18:11:18,266 : INFO : gram7-past-tense: 27.0% (359/1332)\n",
      "2016-12-27 18:11:21,913 : INFO : gram8-plural: 54.4% (540/992)\n",
      "2016-12-27 18:11:24,023 : INFO : gram9-plural-verbs: 25.2% (164/650)\n",
      "2016-12-27 18:11:24,025 : INFO : total: 40.3% (4939/12268)\n",
      "2016-12-27 18:11:24,031 : INFO : loading projection weights from models/text8_ft.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1695/4103, Accuracy: 41.31%\n",
      "Syntactic: 3244/8165, Accuracy: 39.73%\n",
      "\n",
      "Loading FastText embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:11:31,749 : INFO : loaded (71290, 100) matrix from models/text8_ft.vec\n",
      "2016-12-27 18:11:31,977 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FastText (with n-grams):\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:11:33,667 : INFO : capital-common-countries: 62.5% (316/506)\n",
      "2016-12-27 18:11:38,357 : INFO : capital-world: 43.0% (625/1452)\n",
      "2016-12-27 18:11:39,239 : INFO : currency: 12.7% (34/268)\n",
      "2016-12-27 18:11:44,264 : INFO : city-in-state: 18.3% (287/1571)\n",
      "2016-12-27 18:11:45,264 : INFO : family: 43.5% (133/306)\n",
      "2016-12-27 18:11:47,685 : INFO : gram1-adjective-to-adverb: 73.7% (557/756)\n",
      "2016-12-27 18:11:48,692 : INFO : gram2-opposite: 53.9% (165/306)\n",
      "2016-12-27 18:11:52,716 : INFO : gram3-comparative: 64.8% (816/1260)\n",
      "2016-12-27 18:11:54,355 : INFO : gram4-superlative: 53.4% (270/506)\n",
      "2016-12-27 18:11:57,536 : INFO : gram5-present-participle: 54.4% (540/992)\n",
      "2016-12-27 18:12:01,932 : INFO : gram6-nationality-adjective: 93.9% (1288/1371)\n",
      "2016-12-27 18:12:06,220 : INFO : gram7-past-tense: 35.6% (474/1332)\n",
      "2016-12-27 18:12:09,390 : INFO : gram8-plural: 90.1% (894/992)\n",
      "2016-12-27 18:12:11,479 : INFO : gram9-plural-verbs: 59.4% (386/650)\n",
      "2016-12-27 18:12:11,481 : INFO : total: 55.3% (6785/12268)\n",
      "2016-12-27 18:12:11,488 : INFO : loading projection weights from models/text8_wr.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1395/4103, Accuracy: 34.00%\n",
      "Syntactic: 5390/8165, Accuracy: 66.01%\n",
      "\n",
      "\n",
      "Loading Wordrank embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:12:18,908 : INFO : loaded (71290, 100) matrix from models/text8_wr.vec\n",
      "2016-12-27 18:12:18,987 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Wordrank:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:12:20,718 : INFO : capital-common-countries: 84.6% (428/506)\n",
      "2016-12-27 18:12:25,378 : INFO : capital-world: 70.0% (1016/1452)\n",
      "2016-12-27 18:12:26,259 : INFO : currency: 19.0% (51/268)\n",
      "2016-12-27 18:12:31,261 : INFO : city-in-state: 36.0% (565/1571)\n",
      "2016-12-27 18:12:32,263 : INFO : family: 57.8% (177/306)\n",
      "2016-12-27 18:12:34,677 : INFO : gram1-adjective-to-adverb: 15.3% (116/756)\n",
      "2016-12-27 18:12:35,679 : INFO : gram2-opposite: 15.4% (47/306)\n",
      "2016-12-27 18:12:39,683 : INFO : gram3-comparative: 33.8% (426/1260)\n",
      "2016-12-27 18:12:41,314 : INFO : gram4-superlative: 21.1% (107/506)\n",
      "2016-12-27 18:12:44,488 : INFO : gram5-present-participle: 23.8% (236/992)\n",
      "2016-12-27 18:12:48,855 : INFO : gram6-nationality-adjective: 90.2% (1237/1371)\n",
      "2016-12-27 18:12:53,089 : INFO : gram7-past-tense: 26.4% (351/1332)\n",
      "2016-12-27 18:12:56,261 : INFO : gram8-plural: 60.9% (604/992)\n",
      "2016-12-27 18:12:58,352 : INFO : gram9-plural-verbs: 19.7% (128/650)\n",
      "2016-12-27 18:12:58,354 : INFO : total: 44.7% (5489/12268)\n",
      "2016-12-27 18:12:58,361 : INFO : loading projection weights from models/text8_wr_ensemble.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 2237/4103, Accuracy: 54.52%\n",
      "Syntactic: 3252/8165, Accuracy: 39.83%\n",
      "\n",
      "\n",
      "Loading Wordrank ensemble embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:13:05,876 : INFO : loaded (71290, 100) matrix from models/text8_wr_ensemble.vec\n",
      "2016-12-27 18:13:05,973 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Wordrank:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-27 18:13:07,668 : INFO : capital-common-countries: 67.0% (339/506)\n",
      "2016-12-27 18:13:12,303 : INFO : capital-world: 59.0% (856/1452)\n",
      "2016-12-27 18:13:13,193 : INFO : currency: 17.2% (46/268)\n",
      "2016-12-27 18:13:18,175 : INFO : city-in-state: 33.0% (519/1571)\n",
      "2016-12-27 18:13:19,222 : INFO : family: 32.0% (98/306)\n",
      "2016-12-27 18:13:21,637 : INFO : gram1-adjective-to-adverb: 10.3% (78/756)\n",
      "2016-12-27 18:13:22,645 : INFO : gram2-opposite: 10.5% (32/306)\n",
      "2016-12-27 18:13:26,626 : INFO : gram3-comparative: 24.4% (308/1260)\n",
      "2016-12-27 18:13:28,253 : INFO : gram4-superlative: 11.5% (58/506)\n",
      "2016-12-27 18:13:31,412 : INFO : gram5-present-participle: 11.7% (116/992)\n",
      "2016-12-27 18:13:35,744 : INFO : gram6-nationality-adjective: 71.8% (985/1371)\n",
      "2016-12-27 18:13:39,971 : INFO : gram7-past-tense: 17.0% (226/1332)\n",
      "2016-12-27 18:13:43,150 : INFO : gram8-plural: 47.8% (474/992)\n",
      "2016-12-27 18:13:45,243 : INFO : gram9-plural-verbs: 11.7% (76/650)\n",
      "2016-12-27 18:13:45,245 : INFO : total: 34.3% (4211/12268)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1858/4103, Accuracy: 45.28%\n",
      "Syntactic: 2353/8165, Accuracy: 28.82%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading Gensim embeddings')\n",
    "text8_gs = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_gs.vec')\n",
    "print('Accuracy for word2vec:')\n",
    "print_accuracy(text8_gs, word_analogies_file)\n",
    "\n",
    "print('Loading FastText embeddings')\n",
    "text8_ft = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_ft.vec')\n",
    "print('Accuracy for FastText (with n-grams):')\n",
    "print_accuracy(text8_ft, word_analogies_file)\n",
    "\n",
    "print('\\nLoading Wordrank embeddings')\n",
    "text8_wr = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_wr.vec')\n",
    "print('Accuracy for Wordrank:')\n",
    "print_accuracy(text8_wr, word_analogies_file)\n",
    "\n",
    "print('\\nLoading Wordrank ensemble embeddings')\n",
    "text8_wr_ensemble = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_wr_ensemble.vec')\n",
    "print('Accuracy for Wordrank:')\n",
    "print_accuracy(text8_wr_ensemble, word_analogies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With a larger corpus, we observe similar pattern in the accuracies. Here also, wordrank dominates the semantic analogies and fasttext syntactic ones.\n",
    "Though we observe a little performance decrease in Wordrank in case of ensemble embeddings here, so it's good to always try both the cases for evaluations.\n",
    "\n",
    "Now, following graph shows the word frequency effect on analogy task accuracy. For each analogy, the\n",
    "mean frequency of the four words involved is computed, and then bucketed with other analogies having similar mean frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "def calc_parm(model, vocab):\n",
    "    freq = {}\n",
    "    with open(vocab, 'r') as r:\n",
    "        for line in r:\n",
    "            key, val = line.split()\n",
    "            freq[key] = val\n",
    "    mean_freq = {}\n",
    "    with open(word_analogies_file, 'r') as r:\n",
    "        for i, line in enumerate(r):\n",
    "            if ':' not in line:\n",
    "                analogy = tuple(line.split())\n",
    "            else:\n",
    "                continue\n",
    "            try:\n",
    "                mfreq = sum([int(freq[x.lower()]) for x in analogy])/4\n",
    "                mean_freq['a%d'%i] = [analogy, mfreq]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    model = Word2Vec.load_word2vec_format(model)\n",
    "    acc = model.accuracy(word_analogies_file)\n",
    "    sem_correct = [acc[i]['correct'] for i in range(5)]\n",
    "    syn_correct = [acc[i]['correct'] for i in range(5, len(acc)-1)]\n",
    "    tot_correct = [acc[i]['correct'] for i in range(len(acc)-1)]\n",
    "\n",
    "    sem_x, sem_y = calc_axis(sem_correct, mean_freq)\n",
    "    syn_x, syn_y = calc_axis(syn_correct, mean_freq)\n",
    "    total_x, total_y = calc_axis(tot_correct, mean_freq)\n",
    "    return ((sem_x, sem_y), (syn_x, syn_y), (total_x, total_y))\n",
    "\n",
    "def calc_axis(correct, mean_freq):\n",
    "    total_correct = []\n",
    "    for i in range(len(correct)):\n",
    "        for analogy in correct[i]:\n",
    "            total_correct.append(analogy)\n",
    "\n",
    "    dup_mean_freq = copy.deepcopy(mean_freq)\n",
    "    for key, value in dup_mean_freq.iteritems():\n",
    "        value[0] = tuple(x.upper() for x in value[0])\n",
    "        if value[0] in total_correct:\n",
    "            dup_mean_freq[key].append(1)\n",
    "        else:\n",
    "            dup_mean_freq[key].append(0)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    dup_mean_freq = sorted(dup_mean_freq.items(), key=lambda x: x[1][1])\n",
    "    for centre_p in xrange(50, len(dup_mean_freq), 100):\n",
    "        bucket = dup_mean_freq[centre_p-50:centre_p+50]\n",
    "        b_acc = 0\n",
    "        for analogy in bucket:\n",
    "            if analogy[1][2]==1:\n",
    "                b_acc+=1\n",
    "        y.append(b_acc/100)\n",
    "        x.append(np.log(dup_mean_freq[centre_p][1][1]))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "plot_data0 = calc_parm('text8_gs.vec', 'text8.vocab')\n",
    "plot_data1 = calc_parm('text8_wr.vec', 'text8.vocab')\n",
    "plot_data2 = calc_parm('text8_ft.vec', 'text8.vocab')\n",
    "\n",
    "fig = plt.figure(figsize=(7,15))\n",
    "\n",
    "ax = fig.add_subplot('311')\n",
    "ax.plot(plot_data0[0][0], plot_data0[0][1], 'r-', label='Word2Vec')\n",
    "ax.plot(plot_data1[0][0], plot_data1[0][1], 'g--', label='Wordrank')\n",
    "ax.plot(plot_data2[0][0], plot_data2[0][1], 'b:', label='FastText')\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('Log mean frequency')\n",
    "ax.set_title('Semantic Analogies')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot('312')\n",
    "ax.plot(plot_data0[1][0], plot_data0[1][1], 'r-', label='Word2Vec')\n",
    "ax.plot(plot_data1[1][0], plot_data1[1][1], 'g--', label='Wordrank')\n",
    "ax.plot(plot_data2[1][0], plot_data2[1][1], 'b:', label='FastText')\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('Log mean frequency')\n",
    "ax.set_title('Syntactic Analogies')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot('313')\n",
    "ax.plot(plot_data0[2][0], plot_data0[2][1], 'r-', label='Word2Vec')\n",
    "ax.plot(plot_data1[2][0], plot_data1[2][1], 'g--', label='Wordrank')\n",
    "ax.plot(plot_data2[2][0], plot_data2[2][1], 'b:', label='FastText')\n",
    "ax.set_ylabel('Average accuracy')\n",
    "ax.set_xlabel('Log mean frequency')\n",
    "ax.set_title('Total Analogy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note: Wordrank can sometimes produce Nan values while model evaluation, when the embedding vector values get too diverged at some iterations, but it dumps embedding vectors after every few iterations, so you could just load embeddings from a different iteration’s text file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
